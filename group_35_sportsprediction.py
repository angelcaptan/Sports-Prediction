# -*- coding: utf-8 -*-
"""Group 35_SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-OBQknGCwf1j-kDgRUMVH6dAwuu19YTX

#Introduction

**Author: Angel Captan**

**Assignment: Inro to AI Midsem Project**

## Description
In sports prediction, large numbers of factors including the historical performance of the teams, results of matches, and data on players, have to be accounted for to help different stakeholders understand the odds of winning or losing.

In this project, I am tasked to build a model(s) that predict a player's overall rating given the player's profile.

The specific tasks given are;
1. Demonstrate the data preparation & feature extraction process
2. Create feature subsets that show maximum correlation with the dependent variable.
3. Create and train a suitable machine learning model with cross-validation that can predict a player's rating.
4. Measure the model's performance and fine-tune it as a process of optimization.
5. Use the data from another season(players_22) which was not used during the training to test how good is the model.
6. Deploy the model on a simple web page using either (Heroku, Streamlite, or Flask) and upload a video that shows how the model performs on the web page/site.

## Imports and Data Loading

This section of the notebook will be dedicated to installing, loading datasets and libraries
"""

!pip install pandas numpy matplotlib seaborn xgboost sklearn

# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
import pickle

from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, VotingRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score, KFold
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

"""We have the necessary libraries needed, now let's load our dataset"""

#loading datasets
player21_df = pd.read_csv("/content/drive/MyDrive/Cap_Mid/players_21.csv") #for training
player22_df =  pd.read_csv("/content/drive/MyDrive/Cap_Mid/players_22.csv") # for testing

"""# Data Preprocessing

## EDA, Imputation and Encoding

In this section we know that our data is loaded. Therefore we would be performing an exploratory data analysis, identifying features that are important to us, doing imputation and performing encoding on all the necessary columns.

This step is necessary for the transformation of our data since not all columns, rows are needed for the analysis.
"""

#view first few rows and nature of data
player21_df.head()

player22_df.head()

#understand nature of data
player21_df.info()

player22_df.info()

"""From the info decribed we can see that there are a lot of columns to consider for the analysis, so we are going to need to drop some, before finding the needed features we can work with, let's get the number of missing values for each of our dataframes"""

# Checking for missing data
print("Checking sum of missing value for Players 21(Train Data:)")
player21_df.isnull().sum()

print("Checking sum of missing value for Players 22(Test Data:)")
player22_df.isnull().sum()

"""## Dropping Missing Values

Now we are going to drop columns which have 30% of the data missing
"""

total_rows_21 = player21_df.shape[0] #shape for train data

"""Calaculate the 30% threshhold for the two sets"""

threshold_21 = int(0.3 * total_rows_21)

print("The threshold for Players 21 is", threshold_21)

"""Get a list of all columns with a sum of missing values greater than the threshold:"""

columns_to_drop = []
for column in player21_df.columns:
    if player21_df[column].isna().sum() > threshold_21:
        columns_to_drop.append(column)

"""Drop the columns:"""

#run once
player21_df = player21_df.drop(columns=columns_to_drop, axis=0)
player22_df = player22_df.drop(columns=columns_to_drop , axis=0)

"""Let's check info again:"""

#understand nature of data
player21_df.info()

player22_df.info()

"""After reviewing kaggle, reading the data description and looking at things using the data explorer, I came to understand some comuns don't contribute to the overall rating of a player, so we are going to drop those columns too.

Here is a link to exploer the columns in the data: [Data Explorer on Kaggle](https://www.kaggle.com/datasets/stefanoleone992/fifa-22-complete-player-dataset/?select=players_22.csv)
"""

drop_columns = ['sofifa_id','player_url','long_name','dob','body_type','real_face','player_face_url','club_logo_url','club_flag_url','nation_flag_url']

player21_df = player21_df.drop(drop_columns, axis=1)
player22_df = player22_df.drop(drop_columns, axis=1)

"""After a further review, some columns were identified that could be dropped with this justfication. If we look at the `ls` column it is described as the `player attribute playing as LW`.

Such columns are only useful if we wanted to predict a player's effectiveness in playing such a position, so we drop such columns with that description.


Players are normally played in a specific posiion at their clubs which contibutes more to their overall rating, thus columns like `players_positions` which is the `player preferred positions`


Other columns reviewd that can be dropped are;


*   `short_name`
*   `club_joined`
*   `nationality_name`

"""

#drop new identified columnas
drop_r_cols = ['short_name', 'player_positions', 'league_name', 'nationality_name', 'ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk']

player21_df = player21_df.drop(drop_r_cols, axis=1)
player22_df = player22_df.drop(drop_r_cols, axis=1)

"""## Imputation x Encoding

Having dropped columns will many missing values now we do imutation. Imputation is where will fill missing data with certain values.
"""

## Filling missing numeric data with the mean value
num_imputer = SimpleImputer(strategy='mean')

## Filling missing categorical data with the most frequent value
cat_imputer = SimpleImputer(strategy='most_frequent')

# Selecting numerical and categorical features

num_features_21 = player21_df.select_dtypes(include=[np.number]).columns.tolist()
cat_features_21 = player21_df.select_dtypes(include=[np.object]).columns.tolist()

cat_features_21

"""From Kaggle the column `overall` is described as the *player current overall attribute* which transalte to the **the player rating** i.e the crux of this whole project. Thus we remove `overall` since it is our target variable."""

# Removing the target variable from the features
num_features_21.remove('overall')  # 'overall' is the target variable

"""Now we do the imputation:"""

#numeric imputation
player21_df[num_features_21] = num_imputer.fit_transform(player21_df[num_features_21])

#categorical imputation
player21_df[cat_features_21] = cat_imputer.fit_transform(player21_df[cat_features_21])

#numeric imputation
player22_df[num_features_21] = num_imputer.fit_transform(player22_df[num_features_21])

#categorical imputation
player22_df[cat_features_21] = cat_imputer.fit_transform(player22_df[cat_features_21])

player21_df.shape

player22_df.shape

"""Next task is to do encoding. We do this for only categorical columns. We first explored encoding use OneHot Encoding technique, but quickly discovered that we run out of memory so quickly pivoted to encoding using `pd.get_dummies`"""

# Using `get_dummies` for one-hot encoding and dropping the first category
player21_encoded_df = pd.get_dummies(player21_df, columns=cat_features_21, drop_first=True)
player22_encoded_df = pd.get_dummies(player22_df, columns=cat_features_21, drop_first=True)

player21_encoded_df.head()  # display the first few rows to verify the changes

player21_encoded_df.shape

"""Finally let's describe our data set before feature analysis"""

player21_encoded_df.describe()

player22_encoded_df.describe()

"""# Feature Engineering

## Feature Extraction
Now we are going to analyze the dataset to understand which features are important for determining a player's overall rating. We are using feature importance *to* identify necessary features.


Here we are fitting a RandomForestRegressor to obtain feature importances.
"""

# the target variable and features; drop non-numeric columns if necessary
X = player21_encoded_df.drop(columns=['overall'])
y = player21_encoded_df['overall']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=69)

# Create a Random Forest Regressor model
model = RandomForestRegressor(random_state=69)
model.fit(X_train, y_train)

# Get feature importances
importances = model.feature_importances_

# Sort them in descending order
indices = np.argsort(importances)[::-1]

# Let's print out the feature importance ranking
print("Feature ranking:")

for i in range(X.shape[1]):
    print(f"{i + 1}. Feature {X.columns[indices[i]]} ({importances[indices[i]]})")

# Now, let's get the top 10 features
top_features = [X.columns[indices[i]] for i in range(10)]
print("\nTop 10 features with % Contribution:")

for i in range(10):
    print(f"{i + 1}.  {top_features[i]} ({round(importances[indices[i]]*100,2)}%)")

"""From observing the results of the feature importance process I observe the top 5 features contribute a percentage importance of *99.24%*.

Thus my strategy is to use the top 10 features to train so I capture the underlying data patterns even for weak contributing features. Then when testing use the same 5. And, when deployed in the future use the top 5 features for prediction.

Let's see how it Goes. On to Feature subsetting.
"""

top_features = top_features[:5]

print('Features being used for model development are:\n')
top_features

"""## Feature Subset

At this stage our goal is to use the top features we have identified at our feature extraction stage to create subsetted data that we will use to train models.
"""

#Now we subset our X feauture set
X_top_f = X[top_features]
X_top_f

#no need to do for y

"""Now let's scale our features which is our independent variables"""

# Initialize the scaler
scaler = StandardScaler()

# Scale the features
X_scaled = scaler.fit_transform(X_top_f)

# The features are now scaled and ready for training the model.
X_scaled_df = pd.DataFrame(X_scaled, columns=X_top_f.columns)

X_scaled_df.head()

#Saving scaler to use in deployment

with open('scaler.pkl', 'wb') as file:
    pickle.dump(scaler, file)

"""# Training Models

We are now reading to train some models, here we are going to train 3 modes;
1. XGBoost
2. Gradient Boost
3. Random Forest


Lets split data for training
"""

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=84)

"""Now we define a function for training our various models"""

def train_model(model, param_grid, X, y):
    '''
        Trains a model using grid search with cross-validation and returns the best model.
        Parameters:
            model: scikit-learn model
            param_grid: dictionary with parameters to try
            X: features(independent variables)
            y: target(dependent variable)
    '''
    cv = KFold(n_splits=7 , random_state=69, shuffle=True)

    # Grid search with cross-validation
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)
    grid_search.fit(X, y)

    # Results of the grid search
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best score (MAE): {-grid_search.best_score_}")  # We negate the score because grid search maximizes performance (so it negates the scores)

    return grid_search.best_estimator_  # Returns the best model

"""## Model 1: XGBoost"""

print("\nTraining XGBoost...")
xgb_model = xgb.XGBRegressor(random_state=42)
xgb_params = {
    'n_estimators': [100, 500, 1000],
    'learning_rate': [0.1, 0.001, 0.01],
    'max_depth': [3, 5, 9, 15],
    'colsample_bytree': [0.5, 0.75, 1]
}
best_xgb = train_model(xgb_model, xgb_params, X_train, y_train)

"""## Model 2: Gradient Bossting Regressor"""

print("\nTraining Gradient Boosting...")
gbr_model = GradientBoostingRegressor(random_state=63)
gbr_params = {
    'n_estimators': [100, 500, 1000],
    'learning_rate': [0.1, 0.001, 0.01],
    'max_depth': [9, 15]
}
best_gbr = train_model(gbr_model, gbr_params, X_train, y_train)

"""## Model 3: Random Forest Regressor


"""

print("\nTraining Random Forest...")
rf_model = RandomForestRegressor(random_state=39)
rf_params = {
    'n_estimators': [500,1000],
    'max_depth': [12, 15],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}
best_rf = train_model(rf_model, rf_params, X_train, y_train)

"""## Model 4: Ensembled Model

Form discussion in class I have come to understand that, an ensembled model can improve a model's predicitve perfromance. Here I will combine the best versions of my 3 models into a single ensemble model.
"""

# Create an ensemble model
ensemble = VotingRegressor(
    estimators=[
        ('xgb', best_xgb),
        ('gbr', best_gbr),
        ('rf', best_rf)
    ]
)

# Fit model on the training data
print("\nTraining Ensemble Model...")
ensemble.fit(X_train, y_train)

# Predict and evaluate on the training set
train_pred = ensemble.predict(X_train)
train_mae = mean_absolute_error(y_train, train_pred)
print(f"Ensemble model MAE on training set: {train_mae}")

"""Now we have our trained Models. We are moving on to evaluations on the test set to see how they perform. Before Let's save so we don't have to incur cost of training if runtime fails

# Saving Models
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/Cap_Mid"

with open('best_xgb_model.pkl', 'wb') as file:
    pickle.dump(best_xgb, file)

with open('best_gbr_model.pkl', 'wb') as file:
    pickle.dump(best_gbr, file)

with open('best_rf_model.pkl', 'wb') as file:
    pickle.dump(best_rf, file)

with open('ensemble_model.pkl', 'wb') as file:
    pickle.dump(ensemble, file)

"""Test if model saved well"""

with open('ensemble_model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

predictions = loaded_model.predict(X_test)

en_mae = mean_absolute_error(y_test, predictions)

print(f"Ensemble model MAE on test set: {en_mae}")

"""Saved Well

# Evaluation

We are going to do two evaluations, one on the test set seperated from the training data. The other on `Players 22` an unseen dataset similar to the data used to train models

## Test Set Evaluations
"""

print("\nEvaluating XGBoost...")

#predict on test set
pred_xgb = best_xgb.predict(X_test)
xgb_mae = mean_absolute_error(y_test, pred_xgb)

print(f"XGBoost model MAE on test set: {xgb_mae:.2f}")

print("\nEvaluating Gradient Boost...")

#predict on test set
pred_gbr = best_gbr.predict(X_test)
gbr_mae = mean_absolute_error(y_test, pred_gbr)

print(f"Gradient Boost Regressor model MAE on test set: {gbr_mae}:.2f")

print("\nEvaluating Random Forest...")

#predict on test set
pred_rf = best_rf.predict(X_test)
rf_mae = mean_absolute_error(y_test, pred_rf)

print(f"Random Forest Regressor model MAE on test set: {rf_mae:.2f}")

print("\nEvaluating Ensemble...")

#predict on test set
pred_en = ensemble.predict(X_test)
en_mae = mean_absolute_error(y_test, pred_en)

print(f"Ensemble model MAE on test set: {en_mae:.2f}")

"""## Player 22 Evaluations


Here we will test our trained models further on `player22` data, the data has been preprocessed already. We only have to extract just the top features needed
"""

player22_encoded_df['overall']

top_features

player22_encoded_df[top_features]

#Get player 22 info
y_22 = player22_encoded_df['overall']
X_22 = player22_encoded_df[top_features]

#Scale input

X_scaled_22 = scaler.fit_transform(X_22)

# The features are now scaled and ready for training the model.
X22_scaled_df = pd.DataFrame(X_scaled_22, columns=X_22.columns)

#reassign
X_22 = X22_scaled_df

X_22.head()

"""Using saved models here.

### Loading Saved Models
"""

# Commented out IPython magic to ensure Python compatibility.
#move to directory where models are saved
# %cd "/content/drive/MyDrive/Cap_Mid"

with open('best_xgb_model.pkl', 'rb') as file:
    lbest_xgb = pickle.load(file)

with open('best_gbr_model.pkl', 'rb') as file:
    lbest_gbr = pickle.load(file)

with open('best_rf_model.pkl', 'rb') as file:
    lbest_rf = pickle.load(file)

with open('ensemble_model.pkl', 'rb') as file:
    lensemble = pickle.load(file)

"""### Testing"""

print("\nEvaluating XGBoost...")

#predict on test set
pred_xgb = lbest_xgb.predict(X_22)
xgb_mae = mean_absolute_error(y_22, pred_xgb)

print(f"XGBoost model MAE on Players 22 set: {xgb_mae:.2f}")

print("\nEvaluating Random Forest...")

#predict on test set
pred_rf = lbest_rf.predict(X_22)
rf_mae = mean_absolute_error(y_22, pred_rf)

print(f"Random Forest Regressor model MAE on Players 22 set: {rf_mae:.2f}")

print("\nEvaluating Gradient Boost...")

#predict on test set
pred_gbr = lbest_gbr.predict(X_22)
gbr_mae = mean_absolute_error(y_22, pred_gbr)

print(f"Gradient Boost Regressor model MAE on Players 22 set: {gbr_mae:.2f}")

print("\nEvaluating Ensemble...")

#predict on test set
pred_en = lensemble.predict(X_22)
en_mae = mean_absolute_error(y_22, pred_en)

print(f"Ensemble model MAE on Players 22 set: {en_mae:.2f}")

!pip freeze > requirements.txt